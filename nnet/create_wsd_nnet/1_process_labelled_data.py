import os


#
# process the labelled training sets of KAI generated by Java
# the WSD set - and create a single labelled set
# with a single set of labels for each word, record
# those labels (classes) and train a new nnet for all these together
#

base_dir = '/tmp/labelled/'
output_file = os.path.join(base_dir, 'combined-ts.txt')
label_file = os.path.join(base_dir, 'combined-ts.labels.txt')
max_samples_per_file = 250000  # this many items per set max

files = [os.path.join(base_dir, item) for item in os.listdir(base_dir) if item.endswith('labelled-trainingset.csv')]
label_counter = 0
counter_to_label = dict()

with open(output_file, 'w') as writer:
    for file_name in files:
        print("processing " + file_name + ", class @ " + str(label_counter))
        line_counter = 0
        sample_counter = 0
        syn = ""
        with open(file_name) as reader:
            for line in reader:
                line = line.strip()
                if not line.startswith("//"):
                    parts = line.split("|")
                    label = int(parts[0]) + label_counter
                    data = parts[1]
                    writer.write(str(label) + ":" + data + "\n")
                    sample_counter += 1
                    if sample_counter >= max_samples_per_file:
                        break
                else:
                    if line_counter == 0:
                        syn = line[3:line.find(':')]
                    for i in range(100):
                        test_str = "// " + str(i) + ":"
                        if line.startswith(test_str) and len(syn) > 0:
                            counter_to_label[i+label_counter] = syn + ":" + line[3:]

                line_counter += 1

        label_counter = len(counter_to_label)

with open(label_file, 'w') as writer:
    for key in counter_to_label.keys():
        text = counter_to_label[key]
        writer.write(str(key) + ":" + text + "\n")
